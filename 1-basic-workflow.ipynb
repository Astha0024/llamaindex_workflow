{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50333508-ab7e-4c85-9b4b-245f5b392705",
   "metadata": {},
   "source": [
    "### LlamaIndex workflows\n",
    "Abstractions for building complex agentic workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d89ddcc1-8a7b-4490-b9ae-69c5756cb543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-core in ./venv/lib/python3.10/site-packages (0.11.9)\n",
      "Requirement already satisfied: llama-index-llms-openai in ./venv/lib/python3.10/site-packages (0.2.7)\n",
      "Requirement already satisfied: llama-index-utils-workflow in ./venv/lib/python3.10/site-packages (0.2.1)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in ./venv/lib/python3.10/site-packages (from llama-index-core) (3.10.5)\n",
      "Requirement already satisfied: wrapt in ./venv/lib/python3.10/site-packages (from llama-index-core) (1.16.0)\n",
      "Requirement already satisfied: httpx in ./venv/lib/python3.10/site-packages (from llama-index-core) (0.27.2)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in ./venv/lib/python3.10/site-packages (from llama-index-core) (2.0.34)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in ./venv/lib/python3.10/site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in ./venv/lib/python3.10/site-packages (from llama-index-core) (0.7.0)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./venv/lib/python3.10/site-packages (from llama-index-core) (1.2.14)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in ./venv/lib/python3.10/site-packages (from llama-index-core) (4.66.5)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in ./venv/lib/python3.10/site-packages (from llama-index-core) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in ./venv/lib/python3.10/site-packages (from llama-index-core) (8.5.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.10/site-packages (from llama-index-core) (2024.9.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./venv/lib/python3.10/site-packages (from llama-index-core) (3.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./venv/lib/python3.10/site-packages (from llama-index-core) (10.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./venv/lib/python3.10/site-packages (from llama-index-core) (2.32.3)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in ./venv/lib/python3.10/site-packages (from llama-index-core) (1.0.8)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./venv/lib/python3.10/site-packages (from llama-index-core) (4.12.2)\n",
      "Requirement already satisfied: dataclasses-json in ./venv/lib/python3.10/site-packages (from llama-index-core) (0.6.7)\n",
      "Requirement already satisfied: numpy<2.0.0 in ./venv/lib/python3.10/site-packages (from llama-index-core) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in ./venv/lib/python3.10/site-packages (from llama-index-core) (2.9.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./venv/lib/python3.10/site-packages (from llama-index-core) (3.9.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./venv/lib/python3.10/site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.1 in ./venv/lib/python3.10/site-packages (from llama-index-llms-openai) (0.3.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in ./venv/lib/python3.10/site-packages (from llama-index-llms-openai) (1.45.0)\n",
      "Requirement already satisfied: pyvis<0.4.0,>=0.3.2 in ./venv/lib/python3.10/site-packages (from llama-index-utils-workflow) (0.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (24.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (4.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.11.1)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core) (1.4.2)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core) (2024.9.11)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (0.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.10/site-packages (from httpx->llama-index-core) (1.0.5)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.10/site-packages (from httpx->llama-index-core) (3.9)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.10/site-packages (from httpx->llama-index-core) (2024.8.30)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in ./venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core) (2.23.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core) (0.7.0)\n",
      "Requirement already satisfied: ipython>=5.3.0 in ./venv/lib/python3.10/site-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (8.27.0)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in ./venv/lib/python3.10/site-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (3.3.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in ./venv/lib/python3.10/site-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (3.1.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./venv/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./venv/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./venv/lib/python3.10/site-packages (from dataclasses-json->llama-index-core) (3.22.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.2.2)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (3.0.47)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in ./venv/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (5.14.3)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (5.1.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./venv/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.18.0)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (4.9.0)\n",
      "Requirement already satisfied: matplotlib-inline in ./venv/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.1.7)\n",
      "Requirement already satisfied: stack-data in ./venv/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.6.3)\n",
      "Requirement already satisfied: jedi>=0.16 in ./venv/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2>=2.9.6->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.1.5)\n",
      "Requirement already satisfied: packaging>=17.0 in ./venv/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (24.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./venv/lib/python3.10/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.10/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.1.0)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.4.1)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (1.16.0)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/asthapuri/PycharmProjects/llamaindex_workflow/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade llama-index-core llama-index-llms-openai llama-index-utils-workflow python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebcf0cd7-dc6c-4a34-99d8-bfdf1f371ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context\n",
    ")\n",
    "import random\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_index.utils.workflow import draw_most_recent_execution\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4f5e66e-2123-431b-a42e-55f83b086612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex, previously known as GPT Index, is a data framework designed to facilitate the connection between large language models (LLMs) and external data sources. It provides a suite of tools that allow users to ingest, structure, and query data from various sources such as documents, databases, and APIs. The framework is particularly useful for applications that require the integration of LLMs with specific datasets, enabling more efficient and contextually relevant responses.\n",
      "\n",
      "Key features of LlamaIndex include:\n",
      "\n",
      "1. **Data Connectors**: These allow for the ingestion of data from multiple sources, making it easier to gather and preprocess information.\n",
      "2. **Indices**: LlamaIndex offers various types of indices to structure and organize the ingested data, which can then be used to optimize querying.\n",
      "3. **Query Interface**: This feature enables users to perform complex queries on the structured data, leveraging the capabilities of LLMs to provide insightful and accurate answers.\n",
      "\n",
      "Overall, LlamaIndex aims to enhance the functionality of large language models by providing a robust framework for data integration and querying, making it a valuable tool for developers and researchers working with LLMs.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Most workflows are defined as a class. It inherits from the Workflow class\n",
    "# This is a very basic workflow. It starts, does one thing and then stops\n",
    "class OpenAIGenerator(Workflow):\n",
    "    # We can have as many steps as we want\n",
    "    @step()\n",
    "    async def generate(self, ev: StartEvent) -> StopEvent:\n",
    "        llm = OpenAI(model=\"gpt-4o\")\n",
    "        response = await llm.acomplete(ev.query)\n",
    "        return StopEvent(result=str(response))\n",
    "\n",
    "w = OpenAIGenerator(timeout=60, verbose=False)\n",
    "result = await w.run(query=\"What's LlamaIndex?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d95ac060-bcff-4e27-917c-558a3fcbc211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivial_workflow.html\n"
     ]
    }
   ],
   "source": [
    "draw_all_possible_flows(OpenAIGenerator, filename=\"trivial_workflow.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54fc47-d778-4a21-bc4b-7f3c5384e2c0",
   "metadata": {},
   "source": [
    "Lets make a little more complex workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91c02379-efb2-43e2-b5eb-272e072e775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom event\n",
    "class FailedEvent(Event):\n",
    "    error: str\n",
    "\n",
    "# custom event\n",
    "class QueryEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class LoopExampleFlow(Workflow):\n",
    "\n",
    "    # This can take a start event or a query event and emit a failed event or stop event\n",
    "    @step()\n",
    "    async def answer_query(self, ev: StartEvent | QueryEvent ) -> FailedEvent | StopEvent:\n",
    "        query = ev.query\n",
    "        # try to answer the query\n",
    "        random_number = random.randint(0, 1)\n",
    "        if (random_number == 0):\n",
    "            return FailedEvent(error=\"Failed to answer the query.\")\n",
    "        else:\n",
    "            return StopEvent(result=\"The answer to your query\")\n",
    "\n",
    "    @step()\n",
    "    async def improve_query(self, ev: FailedEvent) -> QueryEvent | StopEvent:\n",
    "        # improve the query or decide it can't be fixed\n",
    "        random_number = random.randint(0, 1)\n",
    "        if (random_number == 0):\n",
    "            # llm will improve your query incase of a failed event\n",
    "            return QueryEvent(query=\"Here's a better query.\")\n",
    "        else:\n",
    "            return StopEvent(result=\"Your query can't be fixed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec83d552-020f-4ddd-a120-1c4d85738c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop_workflow.html\n"
     ]
    }
   ],
   "source": [
    "draw_all_possible_flows(LoopExampleFlow, filename=\"loop_workflow.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d14594c0-fed1-4dc5-954d-9ba3d30ec895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step answer_query\n",
      "Step answer_query produced event StopEvent\n",
      "The answer to your query\n"
     ]
    }
   ],
   "source": [
    "# This time, setting verbose as True shows us all the steps\n",
    "l = LoopExampleFlow(timeout=10, verbose=True)\n",
    "result = await l.run(query=\"What's LlamaIndex?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fafe899e-2496-4c42-b6cb-c84a268d899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a global state which allows you to keep arbitrary data or functions around for use by all event handlers.\n",
    "class GlobalExampleFlow(Workflow):\n",
    "\n",
    "    # variable of type context will get passed to the function\n",
    "    @step(pass_context=True)\n",
    "    async def setup(self, ctx: Context, ev: StartEvent) -> QueryEvent:\n",
    "        # load our data here. Attach arbitrary data to context.\n",
    "        ctx.data[\"some_database\"] = [\"value1\",\"value2\",\"value3\"]\n",
    "\n",
    "        return QueryEvent(query=ev.query)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def query(self, ctx: Context, ev: QueryEvent) -> StopEvent:\n",
    "        # use our data with our query\n",
    "        data = ctx.data[\"some_database\"]\n",
    "\n",
    "        result = f\"The answer to your query is {data[1]}\"\n",
    "        return StopEvent(result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0232363b-1c0f-437e-9963-71d5bd7ae21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step setup\n",
      "Step setup produced event QueryEvent\n",
      "Running step query\n",
      "Step query produced event StopEvent\n",
      "The answer to your query is value2\n"
     ]
    }
   ],
   "source": [
    "g = GlobalExampleFlow(timeout=10, verbose=True)\n",
    "result = await g.run(query=\"What's LlamaIndex?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87605c7b-fd5a-43c6-8e02-253404c586cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
